{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_selector import FeatureSelector\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import csv files into dataframes \n",
    "* Make sure to remove the orange category label row in the csv\n",
    "* Also move the original features in front of the new features\n",
    "* Also remove the other targets in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta = ['Reference DOI','Composition ID']\n",
    "coercivity = pd.read_csv('SplitDB\\\\Coercivity7-26.csv').drop(columns=meta)\n",
    "core_loss = pd.read_csv('SplitDB\\\\CoreLoss7-26.csv').drop(columns=meta)\n",
    "curie_temp = pd.read_csv('SplitDB\\\\CurieTemperature7-26.csv').drop(columns=meta)\n",
    "electrical_resistivity = pd.read_csv('SplitDB\\\\ElectricalResistivity7-26.csv').drop(columns=meta)\n",
    "grain_size = pd.read_csv('SplitDB\\\\GrainSize7-26.csv').drop(columns=meta)\n",
    "magnetic_saturation = pd.read_csv('SplitDB\\\\MagneticSaturation7-26.csv').drop(columns=meta)\n",
    "magnetostriction = pd.read_csv('SplitDB\\\\Magnetostriction7-26.csv').drop(columns=meta)\n",
    "permeability = pd.read_csv('SplitDB\\\\Permeability7-26.csv').drop(columns=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dataframes, for reference \n",
    "* coercivity \n",
    "* core_loss \n",
    "* curie_temp \n",
    "* electrical_resistivity \n",
    "* grain_size \n",
    "* magnetic_saturation \n",
    "* magnetostriction \n",
    "* permeability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining training labels\n",
    "coercivity_labels = coercivity['Coercivity']\n",
    "core_loss_labels = core_loss['Core Loss']\n",
    "curie_temp_labels = curie_temp['Curie Temp']\n",
    "electrical_resistivity_labels = electrical_resistivity['Electrical Resistivity']\n",
    "grain_size_labels = grain_size['Grain Diameter']\n",
    "magnetic_saturation_labels = magnetic_saturation['Magnetic Saturation']\n",
    "magnetostriction_labels = magnetostriction['Magnetostriction']\n",
    "permeability_labels = permeability['Permeability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining training features\n",
    "coercivity_features = coercivity.drop(columns=['Coercivity'])\n",
    "core_loss_features = core_loss.drop(columns=['Core Loss'])\n",
    "curie_temp_features = curie_temp.drop(columns=['Curie Temp'])\n",
    "electrical_resistivity_features = electrical_resistivity.drop(columns=['Electrical Resistivity'])\n",
    "grain_size_features = grain_size.drop(columns=['Grain Diameter'])\n",
    "magnetic_saturation_features = magnetic_saturation.drop(columns=['Magnetic Saturation'])\n",
    "magnetostriction_features = magnetostriction.drop(columns=['Magnetostriction'])\n",
    "permeability_features = permeability.drop(columns=['Permeability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building feature selector objects from labels and features\n",
    "fs_coercivity = FeatureSelector(data = coercivity_features, labels = coercivity_labels)\n",
    "fs_core_loss = FeatureSelector(data = core_loss_features, labels = core_loss_labels)\n",
    "fs_curie_temp = FeatureSelector(data = curie_temp_features, labels = curie_temp_labels)\n",
    "fs_electrical_resistivity = FeatureSelector(data = electrical_resistivity_features, labels = electrical_resistivity_labels)\n",
    "fs_grain_size = FeatureSelector(data = grain_size_features, labels = grain_size_labels)\n",
    "fs_magnetic_saturation = FeatureSelector(data = magnetic_saturation_features, labels = magnetic_saturation_labels)\n",
    "fs_magnetostriction = FeatureSelector(data = magnetostriction_features, labels = magnetostriction_labels)\n",
    "fs_permeability = FeatureSelector(data = permeability_features, labels = permeability_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 features with greater than 0.50 missing values.\n",
      "\n",
      "0 features with a single unique value.\n",
      "\n",
      "15 features with a correlation magnitude greater than 0.95.\n",
      "\n",
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's l1: 190.099\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's l1: 143.998\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[717]\tvalid_0's l1: 308.1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[979]\tvalid_0's l1: 256.209\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l1: 251.318\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[876]\tvalid_0's l1: 309.318\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's l1: 239.722\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's l1: 293.234\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's l1: 291.511\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's l1: 304.853\n",
      "\n",
      "14 features with zero importance after one-hot encoding.\n",
      "\n",
      "30 features required for cumulative importance of 0.95 after one hot encoding.\n",
      "29 features do not contribute to cumulative importance of 0.95.\n",
      "\n",
      "40 total features out of 59 identified for removal after one-hot encoding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs_coercivity.identify_all(selection_params = {'missing_threshold': 0.5, 'correlation_threshold': 0.95, \n",
    "                                    'task': 'regression', 'eval_metric': 'l1', \n",
    "                                     'cumulative_importance': 0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 features with greater than 0.50 missing values.\n",
      "\n",
      "0 features with a single unique value.\n",
      "\n",
      "20 features with a correlation magnitude greater than 0.99.\n",
      "\n",
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's l1: 24.9997\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's l1: 14.8009\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[996]\tvalid_0's l1: 33.2468\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l1: 18.6517\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 42.5464\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's l1: 24.0921\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[180]\tvalid_0's l1: 25.6218\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[993]\tvalid_0's l1: 31.6765\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[993]\tvalid_0's l1: 16.7223\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's l1: 22.8711\n",
      "\n",
      "26 features with zero importance after one-hot encoding.\n",
      "\n",
      "15 features required for cumulative importance of 0.99 after one hot encoding.\n",
      "31 features do not contribute to cumulative importance of 0.99.\n",
      "\n",
      "39 total features out of 46 identified for removal after one-hot encoding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs_curie_temp.identify_all(selection_params = {'missing_threshold': 0.5, 'correlation_threshold': 0.99, \n",
    "                                    'task': 'regression', 'eval_metric': 'l1', \n",
    "                                     'cumulative_importance': 0.99})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fs_electrical_resistivity.identify_all(selection_params = {'missing_threshold': 0.5, 'correlation_threshold': 0.95, \n",
    "#                                    'task': 'regression', 'eval_metric': 'l1', \n",
    "#                                     'cumulative_importance': 0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 features with greater than 0.50 missing values.\n",
      "\n",
      "0 features with a single unique value.\n",
      "\n",
      "13 features with a correlation magnitude greater than 0.95.\n",
      "\n",
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[954]\tvalid_0's l1: 3.7626\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[973]\tvalid_0's l1: 2.87316\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[990]\tvalid_0's l1: 2.86748\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[469]\tvalid_0's l1: 2.91933\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's l1: 4.25741\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid_0's l1: 2.40558\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's l1: 4.09086\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's l1: 2.62819\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[281]\tvalid_0's l1: 3.56972\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid_0's l1: 2.35278\n",
      "\n",
      "14 features with zero importance after one-hot encoding.\n",
      "\n",
      "20 features required for cumulative importance of 0.95 after one hot encoding.\n",
      "26 features do not contribute to cumulative importance of 0.95.\n",
      "\n",
      "38 total features out of 46 identified for removal after one-hot encoding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs_grain_size.identify_all(selection_params = {'missing_threshold': 0.5, 'correlation_threshold': 0.95, \n",
    "                                    'task': 'regression', 'eval_metric': 'l1', \n",
    "                                     'cumulative_importance': 0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 features with greater than 0.50 missing values.\n",
      "\n",
      "0 features with a single unique value.\n",
      "\n",
      "28 features with a correlation magnitude greater than 0.95.\n",
      "\n",
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[390]\tvalid_0's l1: 0.0803961\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[427]\tvalid_0's l1: 0.0696476\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[972]\tvalid_0's l1: 0.0798001\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[977]\tvalid_0's l1: 0.0776766\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[259]\tvalid_0's l1: 0.0670665\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[474]\tvalid_0's l1: 0.0517401\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid_0's l1: 0.102569\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[923]\tvalid_0's l1: 0.0831629\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[342]\tvalid_0's l1: 0.0509067\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[903]\tvalid_0's l1: 0.0992002\n",
      "\n",
      "22 features with zero importance after one-hot encoding.\n",
      "\n",
      "26 features required for cumulative importance of 0.99 after one hot encoding.\n",
      "28 features do not contribute to cumulative importance of 0.99.\n",
      "\n",
      "43 total features out of 54 identified for removal after one-hot encoding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs_magnetic_saturation.identify_all(selection_params = {'missing_threshold': 0.5, 'correlation_threshold': 0.95, \n",
    "                                    'task': 'regression', 'eval_metric': 'l1', \n",
    "                                     'cumulative_importance': 0.99})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 features with greater than 0.50 missing values.\n",
      "\n",
      "0 features with a single unique value.\n",
      "\n",
      "19 features with a correlation magnitude greater than 0.99.\n",
      "\n",
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's l1: 2.09474\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l1: 1.36537\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[466]\tvalid_0's l1: 1.17018\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[446]\tvalid_0's l1: 1.76963\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[991]\tvalid_0's l1: 2.07973\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[117]\tvalid_0's l1: 1.30538\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[726]\tvalid_0's l1: 2.13648\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[304]\tvalid_0's l1: 1.9465\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[963]\tvalid_0's l1: 1.90354\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's l1: 2.09967\n",
      "\n",
      "13 features with zero importance after one-hot encoding.\n",
      "\n",
      "23 features required for cumulative importance of 0.99 after one hot encoding.\n",
      "19 features do not contribute to cumulative importance of 0.99.\n",
      "\n",
      "31 total features out of 42 identified for removal after one-hot encoding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs_magnetostriction.identify_all(selection_params = {'missing_threshold': 0.5, 'correlation_threshold': 0.99, \n",
    "                                    'task': 'regression', 'eval_metric': 'l1', \n",
    "                                     'cumulative_importance': 0.99})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 features with greater than 0.50 missing values.\n",
      "\n",
      "0 features with a single unique value.\n",
      "\n",
      "18 features with a correlation magnitude greater than 0.95.\n",
      "\n",
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[605]\tvalid_0's l1: 9854.2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[369]\tvalid_0's l1: 11431.4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l1: 11409\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[404]\tvalid_0's l1: 9822.38\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's l1: 8552.66\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[267]\tvalid_0's l1: 10959\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[273]\tvalid_0's l1: 14619.3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[260]\tvalid_0's l1: 9798.83\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid_0's l1: 6419.74\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid_0's l1: 11305.8\n",
      "\n",
      "22 features with zero importance after one-hot encoding.\n",
      "\n",
      "19 features required for cumulative importance of 0.95 after one hot encoding.\n",
      "34 features do not contribute to cumulative importance of 0.95.\n",
      "\n",
      "42 total features out of 53 identified for removal after one-hot encoding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs_permeability.identify_all(selection_params = {'missing_threshold': 0.5, 'correlation_threshold': 0.95, \n",
    "                                    'task': 'regression', 'eval_metric': 'l1', \n",
    "                                     'cumulative_importance': 0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] methods have been run\n",
      "\n",
      "Removed 40 features including one-hot features.\n",
      "['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] methods have been run\n",
      "\n",
      "Removed 39 features including one-hot features.\n",
      "['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] methods have been run\n",
      "\n",
      "Removed 38 features including one-hot features.\n",
      "['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] methods have been run\n",
      "\n",
      "Removed 43 features including one-hot features.\n",
      "['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] methods have been run\n",
      "\n",
      "Removed 31 features including one-hot features.\n",
      "['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] methods have been run\n",
      "\n",
      "Removed 42 features including one-hot features.\n"
     ]
    }
   ],
   "source": [
    "coercivity_removed_all = fs_coercivity.remove(methods = 'all', keep_one_hot = False)\n",
    "#core_loss_removed_all = fs_core_loss.remove(methods = 'all', keep_one_hot = False)\n",
    "curie_temp_removed_all = fs_curie_temp.remove(methods = 'all', keep_one_hot = False)\n",
    "#electrical_resistivity_removed_all = fs_electrical_resistivity.remove(methods = 'all', keep_one_hot = False)\n",
    "grain_size_removed_all = fs_grain_size.remove(methods = 'all', keep_one_hot = False)\n",
    "magnetic_saturation_removed_all = fs_magnetic_saturation.remove(methods = 'all', keep_one_hot = False)\n",
    "magnetostriction_removed_all = fs_magnetostriction.remove(methods = 'all', keep_one_hot = False)\n",
    "permeability_removed_all = fs_permeability.remove(methods = 'all', keep_one_hot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coercivity_best = list(coercivity_removed_all.iloc[:,:])\n",
    "#core_loss_best = list(core_loss_removed_all.iloc[:,:])\n",
    "curie_temp_best  = list(curie_temp_removed_all.iloc[:,:])\n",
    "#electrical_resistivity_best  = list(electrical_resistivity_removed_all.iloc[:,:])\n",
    "grain_size_best  = list(grain_size_removed_all.iloc[:,:])\n",
    "magnetic_saturation_best  = list(magnetic_saturation_removed_all.iloc[:,:])\n",
    "magnetostriction_best = list(magnetostriction_removed_all.iloc[:,:])\n",
    "permeability_best = list(permeability_removed_all.iloc[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('kept_coercivity.txt', 'w') as file_handler:\n",
    "    for item in coercivity_best:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('kept_core_loss.txt', 'w') as file_handler:\n",
    "#    for item in core_loss_best:\n",
    "#         file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('kept_curie_temp.txt', 'w') as file_handler:\n",
    "    for item in curie_temp_best:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('kept_electrical_resistivity.txt', 'w') as file_handler:\n",
    "#    for item in electrical_resistivity_best:\n",
    "#        kept_electrical_resistivity.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('kept_grain_size.txt', 'w') as file_handler:\n",
    "    for item in grain_size_best:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('kept_magnetic_saturation.txt', 'w') as file_handler:\n",
    "    for item in magnetic_saturation_best:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('kept_magnetostriction.txt', 'w') as file_handler:\n",
    "    for item in magnetostriction_best:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('kept_permability.txt', 'w') as file_handler:\n",
    "    for item in permeability_best:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
